<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>wvideo API documentation</title>
<meta name="description" content="Name Design: wvideo.py â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>wvideo</code></h1>
</header>
<section id="section-intro">
<p>Name Design: wvideo.py</p>
<p>Author: Logan Fortune</p>
<p>Email: logan.fortune@orange.fr</p>
<p>License: Open Source</p>
<p>Client: Wintics</p>
<p>Date: October 2020</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
    Name Design: wvideo.py

    Author: Logan Fortune

    Email: logan.fortune@orange.fr

    License: Open Source

    Client: Wintics

    Date: October 2020
&#34;&#34;&#34;
# Files management
import os
import json
# ML
import torchvision
import torch
# Image Processing
import cv2
import numpy as np

# Download Model
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
# Training dataset : https://github.com/nightrome/cocostuff
# Evaluation Mode
model.eval()
# Use CPU (constraint)
device = torch.device(&#39;cpu&#39;)


def process_video(database_vision, debug=False):
    &#34;&#34;&#34;
    This function aims to get the all the detections
    in a certain zone provided by database_vision.

       Detection via fasterrcnn_resnet50_fpn:

    - boxes (``FloatTensor[N, 4]``)

    - labels (``Int64Tensor[N]``): the predicted labels for each image

    - scores (``Tensor[N]``): the scores or each prediction

    :param debug: Boolean

    :param database_vision: class DatabaseVision

    :return:
    &#34;&#34;&#34;
    # Get the rectangle via database_vision.video_class and database_vision.rect
    # Get the color via database_vision.id_color
    for video_file_name in database_vision.name_files_video:

        video_to_process = cv2.VideoCapture(database_vision.src_path_videos + video_file_name)

        current_rect = get_video_type_by_collision(database_vision.rect, video_file_name)

        assert current_rect is not None

        height_rect, width_rect, \
        absolute_height_marge, absolute_width_marge = get_rectangle(database_vision, current_rect)

        # Debug : print(height_rect, width_rect)
        it_buffer = -1  # iterator to count the number of frames since the last processed.

        # Files Management setup for screenshots
        time_stamp = -1
        lane, hour = filter_file_name(video_file_name)
        assert lane.isnumeric() and hour.isnumeric()

        # &#34;screenshots/highway_&#34;+lane+&#34;/&#34;+hour+&#34;h/&#34;+time_stamp+&#34;.png&#34;
        base_file_name = &#34;./screenshots/highway_&#34; + lane + &#34;/&#34; + hour + &#34;h/&#34;

        try:
            os.makedirs(base_file_name)
        except FileExistsError:
            # directory already exists
            pass

        screenshots_results = {&#39;screenshots&#39;: []}

        while video_to_process.isOpened():

            ret, frame = video_to_process.read()
            time_stamp += 1

            # Skip some frames
            it_buffer += 1
            if it_buffer == database_vision.buffer_size:
                it_buffer = -1

            if ret is True:
                if it_buffer == -1:
                    # Take the region of interest with some margins
                    minor_x = current_rect[0][2][1] - absolute_height_marge
                    minor_y = current_rect[0][0][0] - absolute_width_marge

                    crop_img = frame[
                               minor_x:
                               current_rect[0][2][1] + height_rect + absolute_height_marge,
                               minor_y:
                               current_rect[0][0][0] + width_rect + absolute_width_marge
                               ]

                    # Debug :
                    # cv2.imshow(&#34;cropped&#34;, crop_img)

                    img_process = process_image(crop_img,
                                                img_nn_size=database_vision.do_resizing_method,
                                                normalize=False)

                    detections = run_inference(img_process)

                    boxes = detections[&#34;boxes&#34;]  # boxes
                    labels = detections[&#34;labels&#34;]  # labels
                    scores = detections[&#34;scores&#34;]  # scores

                    tensor_filter = torchvision.ops.nms(boxes, scores, 0.5)
                    # IOU = AREA of Overlap / AREA of the Union

                    resized = None

                    if len(tensor_filter) &gt; 0:
                        one_valid_box_found = False
                        minimum_detection_objects = min(len(tensor_filter),
                                                        database_vision.nb_detection_per_image)
                        for i in range(0, minimum_detection_objects):
                            if labels[tensor_filter[i]] in database_vision.id_object_to_detect \
                                    and scores[tensor_filter[i]] &gt; 0.5:
                                one_valid_box_found = True
                                screenshots_results[&#39;screenshots&#39;].append([
                                    time_stamp,
                                    tuple([int(boxes[tensor_filter[i]][0]) + minor_y,
                                           int(boxes[tensor_filter[i]][1]) + minor_x]),
                                    tuple([int(boxes[tensor_filter[i]][2]) + minor_y,
                                           int(boxes[tensor_filter[i]][3]) + minor_x]),
                                    database_vision.id_color[int(labels[tensor_filter[i]])]
                                ])
                                if debug is True:
                                    rect_pt_1 = tuple([int(boxes[tensor_filter[i]][0]),
                                                       int(boxes[tensor_filter[i]][1])])
                                    rect_pt_2 = tuple([int(boxes[tensor_filter[i]][2]),
                                                       int(boxes[tensor_filter[i]][3])])
                                    resized = cv2.rectangle(img_process.transpose(1, 2, 0),
                                                            rect_pt_1,
                                                            rect_pt_2,
                                                            color=database_vision.id_color[
                                                                int(labels[tensor_filter[i]])
                                                            ], thickness=2)
                        if one_valid_box_found and debug:
                            cv2.imshow(&#34;Filter&#34;, resized)
                if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
                    break
            else:
                break

        with open(base_file_name + &#34;screenshots_result.json&#34;, &#39;w&#39;) as outfile:
            json.dump(screenshots_results, outfile)

        video_to_process.release()
        cv2.destroyAllWindows()


def get_rectangle(database_vision, current_rect):
    &#34;&#34;&#34;
    This function aims to get information about the rectangle.

    :param database_vision:

    :param current_rect:

    :return:
    &#34;&#34;&#34;
    height_rect = current_rect[0][0][1] - current_rect[0][2][1]
    width_rect = current_rect[0][1][0] - current_rect[0][0][0]
    assert height_rect &gt; 0
    assert width_rect &gt; 0

    # Case Study Custom Parameters !!

    # TODO(logan): check that the absolute_height_marge is not too large
    #  according to the image size...
    absolute_height_marge = int(database_vision.marge_rect_for_detection * height_rect / 2)
    absolute_width_marge = int(database_vision.marge_rect_for_detection * width_rect / 2)
    return height_rect, width_rect, absolute_height_marge, absolute_width_marge


def run_inference(img_process):
    &#34;&#34;&#34;
    This function aims to run the inference on the nn selected
    with the img_process image as the input.

    :param img_process: OpenCV image

    :return: torchvision output detection
    &#34;&#34;&#34;
    # run inference on the model and get detections
    tensor_img = torch.FloatTensor([img_process])  # 32-bit floating point
    with torch.no_grad():
        tensor_img = tensor_img.to(device)
        detections = model(tensor_img)[0]
    return detections


def get_video_type_by_collision(video_type_rect, video_name):
    &#34;&#34;&#34;
    This function aims to find what is the right rectangle instantiation
    according to the name of the video.

    :param video_type_rect: class DatabaseVision (self.rect)

    :param video_name: str

    :return: points of the rectangle or None
    &#34;&#34;&#34;
    for video_type in video_type_rect:
        if video_name.find(video_type) != -1:
            return video_type_rect.get(video_type)
    return None


def filter_file_name(file_name):
    &#34;&#34;&#34;
        This function aims to deliver an accurate decomposition of the video file name.
        This function is highly dependent of the format given for the example.

    :param file_name: str

    :return: str - lane number, str - hour number
    &#34;&#34;&#34;
    it_video_format = file_name.find(&#34;.&#34;)
    it_video_lane = file_name.find(&#34;_&#34;)
    it_video_hour = file_name.find(&#34;_&#34;, it_video_lane + 1)
    lane_number = file_name[it_video_lane + 1:it_video_hour]
    hour_number = file_name[it_video_hour + 1:it_video_format - 1]
    return lane_number, hour_number


def process_image(image, img_nn_size=224, normalize=False):
    &#34;&#34;&#34;
        This functions aims to prepare the image for the NN.

        If img_nn_size &lt;= 0 : do not resize !
        Else img_nn_size &gt; 0 : resize !

        All pre-trained models expect input images normalized in the same way,

        i.e. mini-batches of 3-channel RGB images of shape (3 x H x W),
        where H and W are expected to be at least 224.

        The images have to be loaded in to a range of [0, 1]

        and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].

        Theory :

    https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network/

        When you use unnormalized input features,
        the loss function is likely to have very elongated valleys.
        When optimizing with gradient descent,
        this becomes an issue because the gradient will be steep with respect
        some of the parameters.
        That leads to large oscillations in the search space,
        as you are bouncing between steep slopes.
        To compensate, you have to stabilize optimization with small learning rates.

    :param normalize: Bool

    :param image: OpenCV Image

    :param img_nn_size: int

    :return: OpenCV Image
    &#34;&#34;&#34;

    if img_nn_size &gt; 0:
        # rows, columns and channels
        height_img = image.shape[0]
        width_img = image.shape[1]

        # Find the shorter side and resize it to img_nn_size keeping aspect ratio
        # if the height_img &gt; width_img
        if width_img &gt; height_img:
            scale = img_nn_size / width_img
            dim = (int(image.shape[1] * scale), int(image.shape[0] * scale))
            # Constrain the height to be img_nn_size
            resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
        else:
            scale = img_nn_size / height_img
            dim = (int(image.shape[1] * scale), int(image.shape[0] * scale))
            # Constrain the width to be img_nn_size
            resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
    else:
        resized = image

    # DEBUG
    # print(resized.shape, image.shape)
    # cv2.imshow(&#34;cropped&#34;, resized)

    # Convert values to range of 0 to 1 instead of 0-255
    image_filtered = np.array(resized)
    image_filtered = image_filtered / 255

    if normalize is True:
        # Normalize the image according to the spec
        image_filtered -= image_filtered.mean()
        image_filtered /= image_filtered.std()

    # Move color channels to first dimension as expected by PyTorch

    image_filtered = image_filtered.transpose(2, 0, 1)

    return image_filtered


def post_process(database_vision):
    &#34;&#34;&#34;

        This functions aims to get the screenshots.

    :param database_vision: Class DatabaseVision
    :return:
    &#34;&#34;&#34;
    for video_file_name in database_vision.name_files_video:

        video_to_process = cv2.VideoCapture(database_vision.src_path_videos + video_file_name)

        current_rect = get_video_type_by_collision(database_vision.rect, video_file_name)

        assert current_rect is not None

        height_rect = current_rect[0][0][1] - current_rect[0][2][1]
        width_rect = current_rect[0][1][0] - current_rect[0][0][0]
        assert height_rect &gt; 0
        assert width_rect &gt; 0

        # Files Management setup for screenshots
        time_stamp = -1
        lane, hour = filter_file_name(video_file_name)
        assert lane.isnumeric() and hour.isnumeric()

        # &#34;screenshots/highway_&#34;+lane+&#34;/&#34;+hour+&#34;h/&#34;+time_stamp+&#34;.png&#34;
        screenshots_file_name = &#34;./screenshots/highway_&#34; \
                                + lane + &#34;/&#34; \
                                + hour \
                                + &#34;h/screenshots_result.json&#34;
        assert os.path.isfile(screenshots_file_name)

        with open(screenshots_file_name) as json_data:
            screenshots_info = json.load(json_data)

        while video_to_process.isOpened():

            ret, frame = video_to_process.read()
            time_stamp += 1

            rect_to_draw = None
            for elem in screenshots_info[&#34;screenshots&#34;]:
                if elem[0] == time_stamp:
                    rect_to_draw = elem
                    break

            if ret is True and rect_to_draw is not None:
                resized = cv2.rectangle(frame,
                                        tuple(rect_to_draw[1]),
                                        tuple(rect_to_draw[2]),
                                        color=rect_to_draw[3], thickness=2)
                cv2.imshow(&#34;RECT&#34;, resized)
            if ret is False:
                break

            if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
                break</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="wvideo.filter_file_name"><code class="name flex">
<span>def <span class="ident">filter_file_name</span></span>(<span>file_name)</span>
</code></dt>
<dd>
<div class="desc"><p>This function aims to deliver an accurate decomposition of the video file name.
This function is highly dependent of the format given for the example.</p>
<p>:param file_name: str</p>
<p>:return: str - lane number, str - hour number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_file_name(file_name):
    &#34;&#34;&#34;
        This function aims to deliver an accurate decomposition of the video file name.
        This function is highly dependent of the format given for the example.

    :param file_name: str

    :return: str - lane number, str - hour number
    &#34;&#34;&#34;
    it_video_format = file_name.find(&#34;.&#34;)
    it_video_lane = file_name.find(&#34;_&#34;)
    it_video_hour = file_name.find(&#34;_&#34;, it_video_lane + 1)
    lane_number = file_name[it_video_lane + 1:it_video_hour]
    hour_number = file_name[it_video_hour + 1:it_video_format - 1]
    return lane_number, hour_number</code></pre>
</details>
</dd>
<dt id="wvideo.get_rectangle"><code class="name flex">
<span>def <span class="ident">get_rectangle</span></span>(<span>database_vision, current_rect)</span>
</code></dt>
<dd>
<div class="desc"><p>This function aims to get information about the rectangle.</p>
<p>:param database_vision:</p>
<p>:param current_rect:</p>
<p>:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_rectangle(database_vision, current_rect):
    &#34;&#34;&#34;
    This function aims to get information about the rectangle.

    :param database_vision:

    :param current_rect:

    :return:
    &#34;&#34;&#34;
    height_rect = current_rect[0][0][1] - current_rect[0][2][1]
    width_rect = current_rect[0][1][0] - current_rect[0][0][0]
    assert height_rect &gt; 0
    assert width_rect &gt; 0

    # Case Study Custom Parameters !!

    # TODO(logan): check that the absolute_height_marge is not too large
    #  according to the image size...
    absolute_height_marge = int(database_vision.marge_rect_for_detection * height_rect / 2)
    absolute_width_marge = int(database_vision.marge_rect_for_detection * width_rect / 2)
    return height_rect, width_rect, absolute_height_marge, absolute_width_marge</code></pre>
</details>
</dd>
<dt id="wvideo.get_video_type_by_collision"><code class="name flex">
<span>def <span class="ident">get_video_type_by_collision</span></span>(<span>video_type_rect, video_name)</span>
</code></dt>
<dd>
<div class="desc"><p>This function aims to find what is the right rectangle instantiation
according to the name of the video.</p>
<p>:param video_type_rect: class DatabaseVision (self.rect)</p>
<p>:param video_name: str</p>
<p>:return: points of the rectangle or None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_video_type_by_collision(video_type_rect, video_name):
    &#34;&#34;&#34;
    This function aims to find what is the right rectangle instantiation
    according to the name of the video.

    :param video_type_rect: class DatabaseVision (self.rect)

    :param video_name: str

    :return: points of the rectangle or None
    &#34;&#34;&#34;
    for video_type in video_type_rect:
        if video_name.find(video_type) != -1:
            return video_type_rect.get(video_type)
    return None</code></pre>
</details>
</dd>
<dt id="wvideo.post_process"><code class="name flex">
<span>def <span class="ident">post_process</span></span>(<span>database_vision)</span>
</code></dt>
<dd>
<div class="desc"><p>This functions aims to get the screenshots.</p>
<p>:param database_vision: Class DatabaseVision
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def post_process(database_vision):
    &#34;&#34;&#34;

        This functions aims to get the screenshots.

    :param database_vision: Class DatabaseVision
    :return:
    &#34;&#34;&#34;
    for video_file_name in database_vision.name_files_video:

        video_to_process = cv2.VideoCapture(database_vision.src_path_videos + video_file_name)

        current_rect = get_video_type_by_collision(database_vision.rect, video_file_name)

        assert current_rect is not None

        height_rect = current_rect[0][0][1] - current_rect[0][2][1]
        width_rect = current_rect[0][1][0] - current_rect[0][0][0]
        assert height_rect &gt; 0
        assert width_rect &gt; 0

        # Files Management setup for screenshots
        time_stamp = -1
        lane, hour = filter_file_name(video_file_name)
        assert lane.isnumeric() and hour.isnumeric()

        # &#34;screenshots/highway_&#34;+lane+&#34;/&#34;+hour+&#34;h/&#34;+time_stamp+&#34;.png&#34;
        screenshots_file_name = &#34;./screenshots/highway_&#34; \
                                + lane + &#34;/&#34; \
                                + hour \
                                + &#34;h/screenshots_result.json&#34;
        assert os.path.isfile(screenshots_file_name)

        with open(screenshots_file_name) as json_data:
            screenshots_info = json.load(json_data)

        while video_to_process.isOpened():

            ret, frame = video_to_process.read()
            time_stamp += 1

            rect_to_draw = None
            for elem in screenshots_info[&#34;screenshots&#34;]:
                if elem[0] == time_stamp:
                    rect_to_draw = elem
                    break

            if ret is True and rect_to_draw is not None:
                resized = cv2.rectangle(frame,
                                        tuple(rect_to_draw[1]),
                                        tuple(rect_to_draw[2]),
                                        color=rect_to_draw[3], thickness=2)
                cv2.imshow(&#34;RECT&#34;, resized)
            if ret is False:
                break

            if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
                break</code></pre>
</details>
</dd>
<dt id="wvideo.process_image"><code class="name flex">
<span>def <span class="ident">process_image</span></span>(<span>image, img_nn_size=224, normalize=False)</span>
</code></dt>
<dd>
<div class="desc"><p>This functions aims to prepare the image for the NN.</p>
<pre><code>If img_nn_size &lt;= 0 : do not resize !
Else img_nn_size &gt; 0 : resize !

All pre-trained models expect input images normalized in the same way,

i.e. mini-batches of 3-channel RGB images of shape (3 x H x W),
where H and W are expected to be at least 224.

The images have to be loaded in to a range of [0, 1]

and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].

Theory :
</code></pre>
<p><a href="https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network/">https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network/</a></p>
<pre><code>When you use unnormalized input features,
the loss function is likely to have very elongated valleys.
When optimizing with gradient descent,
this becomes an issue because the gradient will be steep with respect
some of the parameters.
That leads to large oscillations in the search space,
as you are bouncing between steep slopes.
To compensate, you have to stabilize optimization with small learning rates.
</code></pre>
<p>:param normalize: Bool</p>
<p>:param image: OpenCV Image</p>
<p>:param img_nn_size: int</p>
<p>:return: OpenCV Image</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_image(image, img_nn_size=224, normalize=False):
    &#34;&#34;&#34;
        This functions aims to prepare the image for the NN.

        If img_nn_size &lt;= 0 : do not resize !
        Else img_nn_size &gt; 0 : resize !

        All pre-trained models expect input images normalized in the same way,

        i.e. mini-batches of 3-channel RGB images of shape (3 x H x W),
        where H and W are expected to be at least 224.

        The images have to be loaded in to a range of [0, 1]

        and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].

        Theory :

    https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network/

        When you use unnormalized input features,
        the loss function is likely to have very elongated valleys.
        When optimizing with gradient descent,
        this becomes an issue because the gradient will be steep with respect
        some of the parameters.
        That leads to large oscillations in the search space,
        as you are bouncing between steep slopes.
        To compensate, you have to stabilize optimization with small learning rates.

    :param normalize: Bool

    :param image: OpenCV Image

    :param img_nn_size: int

    :return: OpenCV Image
    &#34;&#34;&#34;

    if img_nn_size &gt; 0:
        # rows, columns and channels
        height_img = image.shape[0]
        width_img = image.shape[1]

        # Find the shorter side and resize it to img_nn_size keeping aspect ratio
        # if the height_img &gt; width_img
        if width_img &gt; height_img:
            scale = img_nn_size / width_img
            dim = (int(image.shape[1] * scale), int(image.shape[0] * scale))
            # Constrain the height to be img_nn_size
            resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
        else:
            scale = img_nn_size / height_img
            dim = (int(image.shape[1] * scale), int(image.shape[0] * scale))
            # Constrain the width to be img_nn_size
            resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
    else:
        resized = image

    # DEBUG
    # print(resized.shape, image.shape)
    # cv2.imshow(&#34;cropped&#34;, resized)

    # Convert values to range of 0 to 1 instead of 0-255
    image_filtered = np.array(resized)
    image_filtered = image_filtered / 255

    if normalize is True:
        # Normalize the image according to the spec
        image_filtered -= image_filtered.mean()
        image_filtered /= image_filtered.std()

    # Move color channels to first dimension as expected by PyTorch

    image_filtered = image_filtered.transpose(2, 0, 1)

    return image_filtered</code></pre>
</details>
</dd>
<dt id="wvideo.process_video"><code class="name flex">
<span>def <span class="ident">process_video</span></span>(<span>database_vision, debug=False)</span>
</code></dt>
<dd>
<div class="desc"><p>This function aims to get the all the detections
in a certain zone provided by database_vision.</p>
<p>Detection via fasterrcnn_resnet50_fpn:</p>
<ul>
<li>
<p>boxes (<code>FloatTensor[N, 4]</code>)</p>
</li>
<li>
<p>labels (<code>Int64Tensor[N]</code>): the predicted labels for each image</p>
</li>
<li>
<p>scores (<code>Tensor[N]</code>): the scores or each prediction</p>
</li>
</ul>
<p>:param debug: Boolean</p>
<p>:param database_vision: class DatabaseVision</p>
<p>:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_video(database_vision, debug=False):
    &#34;&#34;&#34;
    This function aims to get the all the detections
    in a certain zone provided by database_vision.

       Detection via fasterrcnn_resnet50_fpn:

    - boxes (``FloatTensor[N, 4]``)

    - labels (``Int64Tensor[N]``): the predicted labels for each image

    - scores (``Tensor[N]``): the scores or each prediction

    :param debug: Boolean

    :param database_vision: class DatabaseVision

    :return:
    &#34;&#34;&#34;
    # Get the rectangle via database_vision.video_class and database_vision.rect
    # Get the color via database_vision.id_color
    for video_file_name in database_vision.name_files_video:

        video_to_process = cv2.VideoCapture(database_vision.src_path_videos + video_file_name)

        current_rect = get_video_type_by_collision(database_vision.rect, video_file_name)

        assert current_rect is not None

        height_rect, width_rect, \
        absolute_height_marge, absolute_width_marge = get_rectangle(database_vision, current_rect)

        # Debug : print(height_rect, width_rect)
        it_buffer = -1  # iterator to count the number of frames since the last processed.

        # Files Management setup for screenshots
        time_stamp = -1
        lane, hour = filter_file_name(video_file_name)
        assert lane.isnumeric() and hour.isnumeric()

        # &#34;screenshots/highway_&#34;+lane+&#34;/&#34;+hour+&#34;h/&#34;+time_stamp+&#34;.png&#34;
        base_file_name = &#34;./screenshots/highway_&#34; + lane + &#34;/&#34; + hour + &#34;h/&#34;

        try:
            os.makedirs(base_file_name)
        except FileExistsError:
            # directory already exists
            pass

        screenshots_results = {&#39;screenshots&#39;: []}

        while video_to_process.isOpened():

            ret, frame = video_to_process.read()
            time_stamp += 1

            # Skip some frames
            it_buffer += 1
            if it_buffer == database_vision.buffer_size:
                it_buffer = -1

            if ret is True:
                if it_buffer == -1:
                    # Take the region of interest with some margins
                    minor_x = current_rect[0][2][1] - absolute_height_marge
                    minor_y = current_rect[0][0][0] - absolute_width_marge

                    crop_img = frame[
                               minor_x:
                               current_rect[0][2][1] + height_rect + absolute_height_marge,
                               minor_y:
                               current_rect[0][0][0] + width_rect + absolute_width_marge
                               ]

                    # Debug :
                    # cv2.imshow(&#34;cropped&#34;, crop_img)

                    img_process = process_image(crop_img,
                                                img_nn_size=database_vision.do_resizing_method,
                                                normalize=False)

                    detections = run_inference(img_process)

                    boxes = detections[&#34;boxes&#34;]  # boxes
                    labels = detections[&#34;labels&#34;]  # labels
                    scores = detections[&#34;scores&#34;]  # scores

                    tensor_filter = torchvision.ops.nms(boxes, scores, 0.5)
                    # IOU = AREA of Overlap / AREA of the Union

                    resized = None

                    if len(tensor_filter) &gt; 0:
                        one_valid_box_found = False
                        minimum_detection_objects = min(len(tensor_filter),
                                                        database_vision.nb_detection_per_image)
                        for i in range(0, minimum_detection_objects):
                            if labels[tensor_filter[i]] in database_vision.id_object_to_detect \
                                    and scores[tensor_filter[i]] &gt; 0.5:
                                one_valid_box_found = True
                                screenshots_results[&#39;screenshots&#39;].append([
                                    time_stamp,
                                    tuple([int(boxes[tensor_filter[i]][0]) + minor_y,
                                           int(boxes[tensor_filter[i]][1]) + minor_x]),
                                    tuple([int(boxes[tensor_filter[i]][2]) + minor_y,
                                           int(boxes[tensor_filter[i]][3]) + minor_x]),
                                    database_vision.id_color[int(labels[tensor_filter[i]])]
                                ])
                                if debug is True:
                                    rect_pt_1 = tuple([int(boxes[tensor_filter[i]][0]),
                                                       int(boxes[tensor_filter[i]][1])])
                                    rect_pt_2 = tuple([int(boxes[tensor_filter[i]][2]),
                                                       int(boxes[tensor_filter[i]][3])])
                                    resized = cv2.rectangle(img_process.transpose(1, 2, 0),
                                                            rect_pt_1,
                                                            rect_pt_2,
                                                            color=database_vision.id_color[
                                                                int(labels[tensor_filter[i]])
                                                            ], thickness=2)
                        if one_valid_box_found and debug:
                            cv2.imshow(&#34;Filter&#34;, resized)
                if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
                    break
            else:
                break

        with open(base_file_name + &#34;screenshots_result.json&#34;, &#39;w&#39;) as outfile:
            json.dump(screenshots_results, outfile)

        video_to_process.release()
        cv2.destroyAllWindows()</code></pre>
</details>
</dd>
<dt id="wvideo.run_inference"><code class="name flex">
<span>def <span class="ident">run_inference</span></span>(<span>img_process)</span>
</code></dt>
<dd>
<div class="desc"><p>This function aims to run the inference on the nn selected
with the img_process image as the input.</p>
<p>:param img_process: OpenCV image</p>
<p>:return: torchvision output detection</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_inference(img_process):
    &#34;&#34;&#34;
    This function aims to run the inference on the nn selected
    with the img_process image as the input.

    :param img_process: OpenCV image

    :return: torchvision output detection
    &#34;&#34;&#34;
    # run inference on the model and get detections
    tensor_img = torch.FloatTensor([img_process])  # 32-bit floating point
    with torch.no_grad():
        tensor_img = tensor_img.to(device)
        detections = model(tensor_img)[0]
    return detections</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="wvideo.filter_file_name" href="#wvideo.filter_file_name">filter_file_name</a></code></li>
<li><code><a title="wvideo.get_rectangle" href="#wvideo.get_rectangle">get_rectangle</a></code></li>
<li><code><a title="wvideo.get_video_type_by_collision" href="#wvideo.get_video_type_by_collision">get_video_type_by_collision</a></code></li>
<li><code><a title="wvideo.post_process" href="#wvideo.post_process">post_process</a></code></li>
<li><code><a title="wvideo.process_image" href="#wvideo.process_image">process_image</a></code></li>
<li><code><a title="wvideo.process_video" href="#wvideo.process_video">process_video</a></code></li>
<li><code><a title="wvideo.run_inference" href="#wvideo.run_inference">run_inference</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>