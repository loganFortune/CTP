<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>wvideo API documentation</title>
<meta name="description" content="Name Design: wvideo.py â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>wvideo</code></h1>
</header>
<section id="section-intro">
<p>Name Design: wvideo.py</p>
<p>Author: Logan Fortune</p>
<p>Email: logan.fortune@orange.fr</p>
<p>License: Open Source</p>
<p>Client: Wintics</p>
<p>Date: October 2020</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
    Name Design: wvideo.py

    Author: Logan Fortune

    Email: logan.fortune@orange.fr

    License: Open Source

    Client: Wintics

    Date: October 2020
&#34;&#34;&#34;
# ML
import torchvision
import torch
# Image Processing
import cv2
import numpy as np
# Files management
import os
import json


# Download Model
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
# Training dataset : https://github.com/nightrome/cocostuff
# Evaluation Mode
model.eval()
# Use CPU (constraint)
device = torch.device(&#39;cpu&#39;)


def process_video(database_vision, debug=False):
    &#34;&#34;&#34;
        This function aims to get the all the detections in a certain zone provided by database_vision.

        Detection via fasterrcnn_resnet50_fpn:

        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with values of ``x``
                            between ``0`` and ``W`` and values of ``y`` between ``0`` and ``H``

        - labels (``Int64Tensor[N]``): the predicted labels for each image

        - scores (``Tensor[N]``): the scores or each prediction

    :param database_vision: class DatabaseVision

    :return:
    &#34;&#34;&#34;
    # Get the rectangle via database_vision.video_class and database_vision.rect
    # Get the color via database_vision.id_color
    for video_file_name in database_vision.name_files_video:

        video_to_process = cv2.VideoCapture(database_vision.src_path_videos + video_file_name)

        current_rect = get_video_type_by_collision(database_vision.rect, video_file_name)

        assert current_rect is not None

        height_rect = current_rect[0][0][1] - current_rect[0][2][1]
        width_rect = current_rect[0][1][0] - current_rect[0][0][0]
        assert height_rect &gt; 0
        assert width_rect &gt; 0

        # Case Study Custom Parameters !!

        # TODO(logan): check that the absolute_height_marge is not too large according to the image size...
        marge_rect_for_detection = 0.20  # 30% marge to get a rectangle that is enough to get an accurate detection
        absolute_height_marge = int(marge_rect_for_detection * height_rect / 2)
        absolute_width_marge = int(marge_rect_for_detection * width_rect / 2)

        # Debug : print(height_rect, width_rect)

        buffer_size = 10  # How many frames we skipped ?
        it_buffer = -1  # iterator to count the number of frames since the last processed.

        do_resizing_method = False

        # Files Management setup for screenshots
        time_stamp = -1
        lane, hour = filter_file_name(video_file_name)
        assert lane.isnumeric() and hour.isnumeric()

        # &#34;screenshots/highway_&#34;+lane+&#34;/&#34;+hour+&#34;h/&#34;+time_stamp+&#34;.png&#34;
        try:
            base_file_name = &#34;./screenshots/highway_&#34;+lane+&#34;/&#34;+hour+&#34;h/&#34;
            os.makedirs(base_file_name)
        except FileExistsError:
            # directory already exists
            pass

        screenshots_results = {&#39;screenshots&#39;: []}

        while video_to_process.isOpened():

            ret, frame = video_to_process.read()
            time_stamp += 1

            # Skip some frames
            it_buffer += 1
            if it_buffer == buffer_size:
                it_buffer = -1

            if ret is True:
                if it_buffer == -1:

                    # Take the region of interest with some margins
                    crop_img = frame[
                               current_rect[0][2][1] - absolute_height_marge:
                               current_rect[0][2][1] + height_rect + absolute_height_marge,
                               current_rect[0][0][0] - absolute_width_marge:
                               current_rect[0][0][0] + width_rect + absolute_width_marge
                               ]

                    # Debug :
                    # cv2.imshow(&#34;cropped&#34;, crop_img)

                    img_process = process_image(crop_img, do_resizing_method)

                    # run inference on the model and get detections
                    tensor_img = torch.FloatTensor([img_process])  # 32-bit floating point
                    with torch.no_grad():
                        tensor_img = tensor_img.to(device)
                        detections = model(tensor_img)[0]

                    boxes = detections[&#34;boxes&#34;]  # boxes
                    labels = detections[&#34;labels&#34;]  # labels
                    scores = detections[&#34;scores&#34;]  # scores

                    tensor_filter = torchvision.ops.nms(boxes, scores, 0.5)  # IOU = AREA of Overlap / AREA of the Union

                    one_valid_box_found = False
                    for i in range(0, 3):
                        if labels[tensor_filter[i]] in database_vision.id_object_to_detect \
                                and scores[tensor_filter[i]] &gt; 0.5:
                            one_valid_box_found = True
                            screenshots_results[&#39;screenshots&#39;].append([
                                                    time_stamp,
                                                    tuple([int(boxes[tensor_filter[i]][0]), int(boxes[tensor_filter[i]][1])]),
                                                    tuple([int(boxes[tensor_filter[i]][2]), int(boxes[tensor_filter[i]][3])]),
                                                    database_vision.id_color[int(labels[tensor_filter[i]])]
                                                    ])
                            if debug is True:
                                resized = cv2.rectangle(img_process.transpose(1, 2, 0),
                                                    tuple([int(boxes[tensor_filter[i]][0]), int(boxes[tensor_filter[i]][1])]),
                                                    tuple([int(boxes[tensor_filter[i]][2]), int(boxes[tensor_filter[i]][3])]),
                                                    color=database_vision.id_color[int(labels[tensor_filter[i]])], thickness=2)
                    if one_valid_box_found and debug:
                        cv2.imshow(&#34;Filter&#34;, resized)

                if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
                    break
            else:
                break

        with open(base_file_name + &#34;screenshots_result.txt&#34;, &#39;w&#39;) as outfile:
            json.dump(screenshots_results, outfile)

        video_to_process.release()
        cv2.destroyAllWindows()


def get_video_type_by_collision(video_type_rect, video_name):
    &#34;&#34;&#34;
        This function aims to find what is the right rectangle instantiation according to the name of the video.

    :param video_type_rect: class DatabaseVision (self.rect)

    :param video_name: str

    :return: points of the rectangle or None
    &#34;&#34;&#34;
    for video_type in video_type_rect:
        if video_name.find(video_type) != -1:
            return video_type_rect.get(video_type)
    return None


def filter_file_name(file_name):
    &#34;&#34;&#34;
        This function aims to deliver an accurate decomposition of the video file name.
        This function is highly dependent of the format given for the example.

    :param file_name: str

    :return: str, str
    &#34;&#34;&#34;
    it_video_format = file_name.find(&#34;.&#34;)
    it_video_lane = file_name.find(&#34;_&#34;)
    it_video_hour = file_name.find(&#34;_&#34;, it_video_lane+1)
    return file_name[it_video_lane+1:it_video_hour], file_name[it_video_hour+1:it_video_format-1]


def process_image(image, img_nn_size=224, normalize=False):
    &#34;&#34;&#34;
        This functions aims to prepare the image for the NN.

        If img_nn_size &lt;= 0 : do not resize !
        Else img_nn_size &gt; 0 : resize !

        All pre-trained models expect input images normalized in the same way,

        i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.

        The images have to be loaded in to a range of [0, 1]

        and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].

        Theory :

    https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network/

        When you use unnormalized input features, the loss function is likely to have very elongated valleys.
        When optimizing with gradient descent, this becomes an issue because the gradient will be steep with respect
        some of the parameters.
        That leads to large oscillations in the search space, as you are bouncing between steep slopes.
        To compensate, you have to stabilize optimization with small learning rates.

    :param normalize: Bool

    :param image: OpenCV Image

    :param img_nn_size: int

    :return: OpenCV Image
    &#34;&#34;&#34;

    if img_nn_size &gt; 0:
        # rows, columns and channels
        height_img = image.shape[0]
        width_img = image.shape[1]

        # Find the shorter side and resize it to img_nn_size keeping aspect ratio
        # if the height_img &gt; width_img
        if width_img &gt; height_img:
            scale = img_nn_size / width_img
            dim = (int(image.shape[1] * scale), int(image.shape[0] * scale))
            # Constrain the height to be img_nn_size
            resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
        else:
            scale = img_nn_size / height_img
            dim = (int(image.shape[1] * scale), int(image.shape[0] * scale))
            # Constrain the width to be img_nn_size
            resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
    else:
        resized = image

    # DEBUG
    # print(resized.shape, image.shape)
    # cv2.imshow(&#34;cropped&#34;, resized)

    # Convert values to range of 0 to 1 instead of 0-255
    image_filtered = np.array(resized)
    image_filtered = image_filtered / 255

    if normalize is True:
        # Normalize the image according to the spec
        image_filtered -= image_filtered.mean()
        image_filtered /= image_filtered.std()

    # Move color channels to first dimension as expected by PyTorch

    image_filtered = image_filtered.transpose(2, 0, 1)

    return image_filtered</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="wvideo.filter_file_name"><code class="name flex">
<span>def <span class="ident">filter_file_name</span></span>(<span>file_name)</span>
</code></dt>
<dd>
<div class="desc"><p>This function aims to deliver an accurate decomposition of the video file name.
This function is highly dependent of the format given for the example.</p>
<p>:param file_name: str</p>
<p>:return: str, str</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_file_name(file_name):
    &#34;&#34;&#34;
        This function aims to deliver an accurate decomposition of the video file name.
        This function is highly dependent of the format given for the example.

    :param file_name: str

    :return: str, str
    &#34;&#34;&#34;
    it_video_format = file_name.find(&#34;.&#34;)
    it_video_lane = file_name.find(&#34;_&#34;)
    it_video_hour = file_name.find(&#34;_&#34;, it_video_lane+1)
    return file_name[it_video_lane+1:it_video_hour], file_name[it_video_hour+1:it_video_format-1]</code></pre>
</details>
</dd>
<dt id="wvideo.get_video_type_by_collision"><code class="name flex">
<span>def <span class="ident">get_video_type_by_collision</span></span>(<span>video_type_rect, video_name)</span>
</code></dt>
<dd>
<div class="desc"><p>This function aims to find what is the right rectangle instantiation according to the name of the video.</p>
<p>:param video_type_rect: class DatabaseVision (self.rect)</p>
<p>:param video_name: str</p>
<p>:return: points of the rectangle or None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_video_type_by_collision(video_type_rect, video_name):
    &#34;&#34;&#34;
        This function aims to find what is the right rectangle instantiation according to the name of the video.

    :param video_type_rect: class DatabaseVision (self.rect)

    :param video_name: str

    :return: points of the rectangle or None
    &#34;&#34;&#34;
    for video_type in video_type_rect:
        if video_name.find(video_type) != -1:
            return video_type_rect.get(video_type)
    return None</code></pre>
</details>
</dd>
<dt id="wvideo.process_image"><code class="name flex">
<span>def <span class="ident">process_image</span></span>(<span>image, img_nn_size=224, normalize=False)</span>
</code></dt>
<dd>
<div class="desc"><p>This functions aims to prepare the image for the NN.</p>
<pre><code>If img_nn_size &lt;= 0 : do not resize !
Else img_nn_size &gt; 0 : resize !

All pre-trained models expect input images normalized in the same way,

i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.

The images have to be loaded in to a range of [0, 1]

and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].

Theory :
</code></pre>
<p><a href="https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network/">https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network/</a></p>
<pre><code>When you use unnormalized input features, the loss function is likely to have very elongated valleys.
When optimizing with gradient descent, this becomes an issue because the gradient will be steep with respect
some of the parameters.
That leads to large oscillations in the search space, as you are bouncing between steep slopes.
To compensate, you have to stabilize optimization with small learning rates.
</code></pre>
<p>:param normalize: Bool</p>
<p>:param image: OpenCV Image</p>
<p>:param img_nn_size: int</p>
<p>:return: OpenCV Image</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_image(image, img_nn_size=224, normalize=False):
    &#34;&#34;&#34;
        This functions aims to prepare the image for the NN.

        If img_nn_size &lt;= 0 : do not resize !
        Else img_nn_size &gt; 0 : resize !

        All pre-trained models expect input images normalized in the same way,

        i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.

        The images have to be loaded in to a range of [0, 1]

        and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].

        Theory :

    https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network/

        When you use unnormalized input features, the loss function is likely to have very elongated valleys.
        When optimizing with gradient descent, this becomes an issue because the gradient will be steep with respect
        some of the parameters.
        That leads to large oscillations in the search space, as you are bouncing between steep slopes.
        To compensate, you have to stabilize optimization with small learning rates.

    :param normalize: Bool

    :param image: OpenCV Image

    :param img_nn_size: int

    :return: OpenCV Image
    &#34;&#34;&#34;

    if img_nn_size &gt; 0:
        # rows, columns and channels
        height_img = image.shape[0]
        width_img = image.shape[1]

        # Find the shorter side and resize it to img_nn_size keeping aspect ratio
        # if the height_img &gt; width_img
        if width_img &gt; height_img:
            scale = img_nn_size / width_img
            dim = (int(image.shape[1] * scale), int(image.shape[0] * scale))
            # Constrain the height to be img_nn_size
            resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
        else:
            scale = img_nn_size / height_img
            dim = (int(image.shape[1] * scale), int(image.shape[0] * scale))
            # Constrain the width to be img_nn_size
            resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
    else:
        resized = image

    # DEBUG
    # print(resized.shape, image.shape)
    # cv2.imshow(&#34;cropped&#34;, resized)

    # Convert values to range of 0 to 1 instead of 0-255
    image_filtered = np.array(resized)
    image_filtered = image_filtered / 255

    if normalize is True:
        # Normalize the image according to the spec
        image_filtered -= image_filtered.mean()
        image_filtered /= image_filtered.std()

    # Move color channels to first dimension as expected by PyTorch

    image_filtered = image_filtered.transpose(2, 0, 1)

    return image_filtered</code></pre>
</details>
</dd>
<dt id="wvideo.process_video"><code class="name flex">
<span>def <span class="ident">process_video</span></span>(<span>database_vision, debug=False)</span>
</code></dt>
<dd>
<div class="desc"><p>This function aims to get the all the detections in a certain zone provided by database_vision.</p>
<pre><code>Detection via fasterrcnn_resnet50_fpn:

- boxes (&lt;code&gt;FloatTensor\[N, 4]&lt;/code&gt;): the predicted boxes in &lt;code&gt;\[x1, y1, x2, y2]&lt;/code&gt; format, with values of &lt;code&gt;x&lt;/code&gt;
                    between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; and values of &lt;code&gt;y&lt;/code&gt; between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt;

- labels (&lt;code&gt;Int64Tensor\[N]&lt;/code&gt;): the predicted labels for each image

- scores (&lt;code&gt;Tensor\[N]&lt;/code&gt;): the scores or each prediction
</code></pre>
<p>:param database_vision: class DatabaseVision</p>
<p>:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_video(database_vision, debug=False):
    &#34;&#34;&#34;
        This function aims to get the all the detections in a certain zone provided by database_vision.

        Detection via fasterrcnn_resnet50_fpn:

        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with values of ``x``
                            between ``0`` and ``W`` and values of ``y`` between ``0`` and ``H``

        - labels (``Int64Tensor[N]``): the predicted labels for each image

        - scores (``Tensor[N]``): the scores or each prediction

    :param database_vision: class DatabaseVision

    :return:
    &#34;&#34;&#34;
    # Get the rectangle via database_vision.video_class and database_vision.rect
    # Get the color via database_vision.id_color
    for video_file_name in database_vision.name_files_video:

        video_to_process = cv2.VideoCapture(database_vision.src_path_videos + video_file_name)

        current_rect = get_video_type_by_collision(database_vision.rect, video_file_name)

        assert current_rect is not None

        height_rect = current_rect[0][0][1] - current_rect[0][2][1]
        width_rect = current_rect[0][1][0] - current_rect[0][0][0]
        assert height_rect &gt; 0
        assert width_rect &gt; 0

        # Case Study Custom Parameters !!

        # TODO(logan): check that the absolute_height_marge is not too large according to the image size...
        marge_rect_for_detection = 0.20  # 30% marge to get a rectangle that is enough to get an accurate detection
        absolute_height_marge = int(marge_rect_for_detection * height_rect / 2)
        absolute_width_marge = int(marge_rect_for_detection * width_rect / 2)

        # Debug : print(height_rect, width_rect)

        buffer_size = 10  # How many frames we skipped ?
        it_buffer = -1  # iterator to count the number of frames since the last processed.

        do_resizing_method = False

        # Files Management setup for screenshots
        time_stamp = -1
        lane, hour = filter_file_name(video_file_name)
        assert lane.isnumeric() and hour.isnumeric()

        # &#34;screenshots/highway_&#34;+lane+&#34;/&#34;+hour+&#34;h/&#34;+time_stamp+&#34;.png&#34;
        try:
            base_file_name = &#34;./screenshots/highway_&#34;+lane+&#34;/&#34;+hour+&#34;h/&#34;
            os.makedirs(base_file_name)
        except FileExistsError:
            # directory already exists
            pass

        screenshots_results = {&#39;screenshots&#39;: []}

        while video_to_process.isOpened():

            ret, frame = video_to_process.read()
            time_stamp += 1

            # Skip some frames
            it_buffer += 1
            if it_buffer == buffer_size:
                it_buffer = -1

            if ret is True:
                if it_buffer == -1:

                    # Take the region of interest with some margins
                    crop_img = frame[
                               current_rect[0][2][1] - absolute_height_marge:
                               current_rect[0][2][1] + height_rect + absolute_height_marge,
                               current_rect[0][0][0] - absolute_width_marge:
                               current_rect[0][0][0] + width_rect + absolute_width_marge
                               ]

                    # Debug :
                    # cv2.imshow(&#34;cropped&#34;, crop_img)

                    img_process = process_image(crop_img, do_resizing_method)

                    # run inference on the model and get detections
                    tensor_img = torch.FloatTensor([img_process])  # 32-bit floating point
                    with torch.no_grad():
                        tensor_img = tensor_img.to(device)
                        detections = model(tensor_img)[0]

                    boxes = detections[&#34;boxes&#34;]  # boxes
                    labels = detections[&#34;labels&#34;]  # labels
                    scores = detections[&#34;scores&#34;]  # scores

                    tensor_filter = torchvision.ops.nms(boxes, scores, 0.5)  # IOU = AREA of Overlap / AREA of the Union

                    one_valid_box_found = False
                    for i in range(0, 3):
                        if labels[tensor_filter[i]] in database_vision.id_object_to_detect \
                                and scores[tensor_filter[i]] &gt; 0.5:
                            one_valid_box_found = True
                            screenshots_results[&#39;screenshots&#39;].append([
                                                    time_stamp,
                                                    tuple([int(boxes[tensor_filter[i]][0]), int(boxes[tensor_filter[i]][1])]),
                                                    tuple([int(boxes[tensor_filter[i]][2]), int(boxes[tensor_filter[i]][3])]),
                                                    database_vision.id_color[int(labels[tensor_filter[i]])]
                                                    ])
                            if debug is True:
                                resized = cv2.rectangle(img_process.transpose(1, 2, 0),
                                                    tuple([int(boxes[tensor_filter[i]][0]), int(boxes[tensor_filter[i]][1])]),
                                                    tuple([int(boxes[tensor_filter[i]][2]), int(boxes[tensor_filter[i]][3])]),
                                                    color=database_vision.id_color[int(labels[tensor_filter[i]])], thickness=2)
                    if one_valid_box_found and debug:
                        cv2.imshow(&#34;Filter&#34;, resized)

                if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
                    break
            else:
                break

        with open(base_file_name + &#34;screenshots_result.txt&#34;, &#39;w&#39;) as outfile:
            json.dump(screenshots_results, outfile)

        video_to_process.release()
        cv2.destroyAllWindows()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="wvideo.filter_file_name" href="#wvideo.filter_file_name">filter_file_name</a></code></li>
<li><code><a title="wvideo.get_video_type_by_collision" href="#wvideo.get_video_type_by_collision">get_video_type_by_collision</a></code></li>
<li><code><a title="wvideo.process_image" href="#wvideo.process_image">process_image</a></code></li>
<li><code><a title="wvideo.process_video" href="#wvideo.process_video">process_video</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>